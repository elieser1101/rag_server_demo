# Description
A simple RAG server using ChromaDB as a vector store and LangChain to pipe operations for the app

## Requirements
- vector embeddings generated by [vector_store_manager](https://github.com/elieser1101/k8s_vector_store_manager), be mindfull of running it with the `GENERATE_EMBEDINGS=True` 
- `OPENAI_API_KEY` we use open AI as LLM
- docker
- curl

## Usage

### Build 
```
docker build -t rag_server_demo .
```

### Run
```
docker run -e OPENAI_API_KEY="<your-openai-key>" --volume /tmp/testcontainer:/tmp/testcontainer -p 8000:8000 --name rag_server_api rag_server_demo
```
### Delete las running container
```
docker rm $(docker stop $(docker ps -a -q --filter name=rag_server_api --format="{{.ID}}"))
```
### Run custom query
```
curl -X 'POST' \
  'http://0.0.0.0:8000/completion' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  "query": "what is an endpoint?"
}' | jq
```
